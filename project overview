# Text Classification Using Pre-trained Word Embeddings #
This project leverages pre-trained word embeddings and machine learning techniques to classify text data based on sentiment and other properties. The primary objective is to preprocess and classify lengthy comments into specific categories using logistic regression.

Project Overview
This project:

Preprocesses text data, removing unwanted characters, extra spaces, and handling missing values.
Computes vectorized word embeddings for each comment using pre-trained word embeddings from gensim.
Trains a logistic regression model to classify comments based on their sentiment.
Evaluates the model using metrics like precision, recall, and F1-score.
The dataset used in this project includes lengthy comments from various topics, providing a robust testbed for text classification using machine learning.

Features
Data Preprocessing: Handles missing and duplicate data, cleans text, and prepares it for embedding.
Pre-trained Word Embeddings: Utilizes pre-trained word vectors (word2vec-google-news-300) to create embeddings for comments.
Machine Learning: Implements logistic regression for text classification.
Evaluation: Reports performance metrics, including precision, recall, and F1-score.
Technologies Used
Programming Language: Python
Libraries:
pandas (data manipulation)
numpy (numerical operations)
re (regex for text preprocessing)
gensim (pre-trained embeddings)
scikit-learn (machine learning and evaluation)
tqdm (progress visualization)

Usage
Place your dataset in the appropriate directory and update the csv file path in the script.
Run the script to preprocess the data, compute word embeddings, and train the model.
View the model's performance metrics in the terminal.
